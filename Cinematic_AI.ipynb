{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C7MrjrVue1f"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers pinecone transformers pandas torch rank_bm25 nltk --quiet\n",
        "!pip install nest_asyncio --quiet\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "Py_96yozzuRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRTagtXKf4f9"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi  # Check GPU memory\n",
        "!free -h     # Check RAM usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLZEoPj55ylJ"
      },
      "source": [
        "#Configuration & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVUR5aHM6_2N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import ast\n",
        "import nltk\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import gc\n",
        "import time\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eEmZxD6BIKF"
      },
      "outputs": [],
      "source": [
        "# Free up memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "CONFIG = {\n",
        "    # Core components\n",
        "    \"index_name\": \"cinematicAI-rag\",\n",
        "    \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"reranker\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"llm_model\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "\n",
        "    # Processing parameters\n",
        "    \"chunk_size\": 600,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"top_k_retrieve\": 50,\n",
        "    \"top_k_final\": 12,\n",
        "\n",
        "    # Response generation\n",
        "    \"max_response_tokens\": 800,\n",
        "    \"min_imdb_rating\": 6.5,\n",
        "    \"default_year_range\": 10,\n",
        "\n",
        "    # System settings\n",
        "    \"pinecone_env\": \"us-east1-gcp\",\n",
        "    \"upload_batch_size\": 150,\n",
        "    \"rate_limit_delay\": 12,  # Seconds between Pinecone batches\n",
        "\n",
        "    \"bm25_path\": os.path.join(os.getenv(\"DATA_DIR\", \".\"), \"bm25_index.pkl\"),\n",
        "    \"metadata_path\": os.path.join(os.getenv(\"DATA_DIR\", \".\"), \"processed_metadata.parquet\"),\n",
        "}\n",
        "\n",
        "# Hardware configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmCLE9uSoRsV"
      },
      "source": [
        "#Devices and Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYpDjb_AoO-D"
      },
      "outputs": [],
      "source": [
        "nltk.download(['stopwords', 'punkt', 'punkt_tab'], quiet=True)\n",
        "reranker = CrossEncoder(CONFIG[\"reranker\"], max_length=512).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"llm_model\"])\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loading with Error Recovery\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        CONFIG[\"llm_model\"],\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        use_cache=False  # Disable cache at load time\n",
        "    )\n",
        "    model.eval()\n",
        "except Exception as e:\n",
        "    print(f\"Model loading failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "del quantization_config\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "I5LqwOharHcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58L912Ut5-lh"
      },
      "source": [
        "#Data Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT5Jgq7fZ0YB"
      },
      "outputs": [],
      "source": [
        "# Data Processing Pipeline\n",
        "def clean_text(text):\n",
        "    \"\"\"Sanitize text input with robust error handling\"\"\"\n",
        "    try:\n",
        "        text = str(text)\n",
        "        # Remove HTML, URLs, and special characters\n",
        "        text = re.sub(r'<[^>]+>|http\\S+|[^a-zA-Z\\s]', '', text)\n",
        "        return text.lower().strip()[:2000]  # Limit to prevent OOM\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def process_data(df):\n",
        "    \"\"\"Transform raw data into structured chunks with metadata\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert stringified lists to actual lists\n",
        "    list_fields = ['genres', 'cast']\n",
        "    for field in list_fields:\n",
        "        df[field] = df[field].apply(\n",
        "            lambda x: ast.literal_eval(str(x)) if pd.notnull(x) else [])\n",
        "\n",
        "    # Calculate review helpfulness metrics\n",
        "    df[['helpful_votes', 'total_votes']] = (\n",
        "        df['helpful'].str.split('/', expand=True)\n",
        "        .apply(pd.to_numeric, errors='coerce'))\n",
        "    df['helpful_ratio'] = np.where(\n",
        "        df['total_votes'] > 0,\n",
        "        df['helpful_votes'] / df['total_votes'],\n",
        "        0.0\n",
        "    )\n",
        "\n",
        "    # Text preprocessing\n",
        "    df['clean_text'] = df['review_text'].apply(clean_text)\n",
        "\n",
        "    # Generate chunks with full metadata\n",
        "    chunks = []\n",
        "    for _, row in df.iterrows():\n",
        "        words = row['clean_text'].split()\n",
        "        for i in range(0, len(words), CONFIG['chunk_size'] - CONFIG['chunk_overlap']):\n",
        "            chunk = ' '.join(words[i:i+CONFIG['chunk_size']])\n",
        "\n",
        "            chunks.append({\n",
        "                \"text\": chunk,\n",
        "                \"title\": str(row['title']),\n",
        "                \"year\": int(row['year']) if pd.notnull(row['year']) else 0,\n",
        "                \"genres\": ', '.join(row['genres']),\n",
        "                \"imdb_rating\": float(row['imdb_rating']) if pd.notnull(row['imdb_rating']) else 0.0,\n",
        "                \"director\": \"Unknown\" if pd.isnull(row['director']) else str(row['director']),\n",
        "                \"cast\": ', '.join(row['cast']),\n",
        "                \"user_rating\": float(row['user_rating']) if pd.notnull(row['user_rating']) else 0.0,\n",
        "                \"helpful_ratio\": round(row['helpful_ratio'], 2),\n",
        "                \"raw_helpful\": f\"{int(row['helpful_votes'])}/{int(row['total_votes'])}\"\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame({\"metadata\": chunks})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDsxFBy76Ml9"
      },
      "source": [
        "#Vector Store Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFKW_81YuCIA"
      },
      "outputs": [],
      "source": [
        "def initialize_pinecone():\n",
        "    # pc = Pinecone(api_key=\"PINECONE_API_KEY\")\n",
        "    import os\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # Load environment variables from .env file\n",
        "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "    if CONFIG[\"index_name\"] not in pc.list_indexes().names():\n",
        "        print(\"Creating new index...\")\n",
        "        encoder = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
        "        pc.create_index(\n",
        "            name=CONFIG[\"index_name\"],\n",
        "            dimension=encoder.get_sentence_embedding_dimension(),\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "    return pc.Index(CONFIG[\"index_name\"])\n",
        "\n",
        "def upload_to_pinecone(index, chunks_df, start_id=0):\n",
        "    \"\"\"Batch upload with rate limiting and error handling\"\"\"\n",
        "    encoder = SentenceTransformer(CONFIG[\"embedding_model\"]).to(device)\n",
        "    current_id = start_id\n",
        "\n",
        "    for batch_idx in range(0, len(chunks_df), CONFIG[\"upload_batch_size\"]):\n",
        "        batch = chunks_df.iloc[batch_idx:batch_idx+CONFIG[\"upload_batch_size\"]]\n",
        "        texts = batch['metadata'].apply(lambda x: x['text']).tolist()\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings with proper type handling\n",
        "            with torch.no_grad(), torch.amp.autocast(device_type='cuda'):\n",
        "                embeddings = encoder.encode(\n",
        "                    texts,\n",
        "                    batch_size=32,\n",
        "                    show_progress_bar=True,\n",
        "                    convert_to_tensor=True  # Ensure tensor output\n",
        "                )\n",
        "                # Move to CPU if needed and convert to numpy\n",
        "                if isinstance(embeddings, torch.Tensor):\n",
        "                    embeddings = embeddings.cpu().numpy()\n",
        "                else:\n",
        "                    embeddings = np.array(embeddings)\n",
        "\n",
        "            # Prepare batch records\n",
        "            records = [{\n",
        "                \"id\": f\"chunk-{current_id + idx}\",\n",
        "                \"values\": emb.tolist(),\n",
        "                \"metadata\": {k: str(v) if not isinstance(v, (int, float)) else float(v)\n",
        "                           for k, v in row['metadata'].items()}\n",
        "            } for idx, ((_, row), emb) in enumerate(zip(batch.iterrows(), embeddings))]\n",
        "\n",
        "            # Upsert with rate limiting\n",
        "            index.upsert(vectors=records)\n",
        "            current_id += len(records)\n",
        "\n",
        "            print(f\"Successfully uploaded batch {batch_idx//CONFIG['upload_batch_size']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Batch {batch_idx} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(CONFIG[\"rate_limit_delay\"])\n",
        "\n",
        "    return current_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lHXZY4g6h0q"
      },
      "source": [
        "# RAG Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global dictionary to track user preferences\n",
        "user_preferences = {\n",
        "    \"watch_history\": [],\n",
        "    \"genre_preferences\": {},\n",
        "    \"director_preferences\": {},\n",
        "    \"actor_preferences\": {},\n",
        "    \"conversation_history\": []\n",
        "}\n",
        "\n",
        "def update_user_preferences(query, response, context):\n",
        "    \"\"\"Update user preferences based on interaction\"\"\"\n",
        "    # Update genre preferences from context\n",
        "    for ctx in context:\n",
        "        if 'genres' in ctx:\n",
        "            genres = [g.strip() for g in ctx.get('genres', '').split(',')]\n",
        "            for genre in genres:\n",
        "                if genre:\n",
        "                    user_preferences[\"genre_preferences\"][genre] = user_preferences[\"genre_preferences\"].get(genre, 0) + 1\n",
        "\n",
        "        # Add to watch history\n",
        "        title = ctx.get('title', '')\n",
        "        if title and title not in user_preferences[\"watch_history\"]:\n",
        "            user_preferences[\"watch_history\"].append(title)\n",
        "\n",
        "    # Extract the actual response content (not the system prompt)\n",
        "    actual_response = response\n",
        "    if \"You are CinematicAI\" in response and \"\\n\\n\" in response:\n",
        "        # Skip the system prompt part\n",
        "        actual_response = response.split(\"\\n\\n\", 1)[-1].strip()\n",
        "\n",
        "    # Add to conversation history with proper truncation\n",
        "    user_preferences[\"conversation_history\"].append({\n",
        "        \"query\": query,\n",
        "        \"response\": actual_response[:200]  # Store more of the response for context\n",
        "    })\n",
        "\n",
        "    # Keep the most recent 3 exchanges\n",
        "    if len(user_preferences[\"conversation_history\"]) > 3:\n",
        "        user_preferences[\"conversation_history\"] = user_preferences[\"conversation_history\"][-3:]"
      ],
      "metadata": {
        "id": "i_m_tyuBHODD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_query_filters(query):\n",
        "    \"\"\"Parse natural language queries into structured filters with helpfulness consideration\"\"\"\n",
        "    filters = {\"year\": {}, \"imdb_rating\": {}}\n",
        "\n",
        "    # Year extraction\n",
        "    year_match = re.search(r'\\b(20\\d{2})s?\\\\b', query)\n",
        "    current_year = datetime.now().year\n",
        "    if year_match:\n",
        "        target_year = int(year_match.group(1))\n",
        "        filters[\"year\"] = {\"$gte\": target_year - 2, \"$lte\": target_year + 2}\n",
        "    else:\n",
        "        filters[\"year\"] = {\"$gte\": current_year - CONFIG[\"default_year_range\"]}\n",
        "\n",
        "    # Rating extraction\n",
        "    rating_keywords = {\n",
        "        'excellent': 8.5, 'great': 8.0, 'good': 7.5,\n",
        "        'decent': 7.0, 'average': 6.5\n",
        "    }\n",
        "    for term, threshold in rating_keywords.items():\n",
        "        if term in query.lower():\n",
        "            filters[\"imdb_rating\"] = {\"$gte\": threshold}\n",
        "            break\n",
        "    else:\n",
        "        filters[\"imdb_rating\"] = {\"$gte\": CONFIG[\"min_imdb_rating\"]}\n",
        "\n",
        "    # People extraction\n",
        "    people = []\n",
        "    if 'directed by' in query:\n",
        "        people.extend(re.findall(r'directed by (\\w+ \\w+)', query, re.I))\n",
        "    if 'starring' in query:\n",
        "        people.extend(re.findall(r'starring (\\w+ \\w+)', query, re.I))\n",
        "\n",
        "    if people:\n",
        "        # Use simple equality match instead of unsupported $contains_any\n",
        "        if len(people) == 1:\n",
        "            filters[\"$or\"] = [\n",
        "                {\"director\": people[0]},\n",
        "                {\"cast\": people[0]}\n",
        "            ]\n",
        "\n",
        "    # If the query mentions reviews or opinions, prioritize helpful reviews\n",
        "    if any(term in query.lower() for term in ['review', 'opinion', 'liked', 'popular', 'recommended', 'highly rated']):\n",
        "        filters[\"helpful_ratio\"] = {\"$gte\": 0.7}\n",
        "\n",
        "    # Detect if query is about TV shows (but without using preferences for now)\n",
        "    is_tv_query = any(term in query.lower() for term in\n",
        "                     ['tv', 'show', 'series', 'episode', 'season'])\n",
        "    if is_tv_query:\n",
        "        filters[\"content_type\"] = \"tv_show\"\n",
        "\n",
        "    return filters"
      ],
      "metadata": {
        "id": "VQK9xXMIu2Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEawtTvlcpui"
      },
      "outputs": [],
      "source": [
        "def initialize_bm25(chunks_df):\n",
        "    \"\"\"Initialize BM25 index for keyword-based retrieval\n",
        "    Required for hybrid search architecture\n",
        "    \"\"\"\n",
        "    # Extract texts from all metadata entries\n",
        "    texts = [row['metadata']['text'] for _, row in chunks_df.iterrows()]\n",
        "\n",
        "    # Enhanced tokenization with stopword removal and lemmatization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenized = [\n",
        "        [t.lower() for t in word_tokenize(text)\n",
        "         if t not in stop_words and len(t) > 2]  # Filter short tokens\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "    return BM25Okapi(tokenized)\n",
        "\n",
        "def normalize_metadata(metadata):\n",
        "    \"\"\"Universal metadata format converter\n",
        "    Critical for handling different Pinecone response formats\n",
        "    \"\"\"\n",
        "    if isinstance(metadata, list):\n",
        "        # Handle list format: [{'key': 'year', 'value': 2010}, ...]\n",
        "        return {item.get('key', 'unknown'): item.get('value', '')\n",
        "                for item in metadata}\n",
        "    elif isinstance(metadata, dict):\n",
        "        # Handle direct dict format\n",
        "        return {k: v for k, v in metadata.items() if not k.startswith('_')}\n",
        "    else:\n",
        "        # Fallback for unexpected formats\n",
        "        return dict(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3N8GMPSqbx4"
      },
      "outputs": [],
      "source": [
        "async def hybrid_search(query, index, bm25_index, chunks_df):\n",
        "    \"\"\"Enhanced retrieval with semantic + lexical search\"\"\"\n",
        "    # Query normalization\n",
        "    query = re.sub(r\"(\\w)([A-Z][a-z])\", r\"\\1 \\2\", query)\n",
        "    query = re.sub(r'\\s+', ' ', query).strip()\n",
        "\n",
        "    # Extract basic filters (avoiding unsupported operators)\n",
        "    filters = extract_query_filters(query)\n",
        "\n",
        "    # Vector search with metadata filtering\n",
        "    encoder = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
        "    vector_results = index.query(\n",
        "        vector=encoder.encode(query).tolist(),\n",
        "        top_k=CONFIG[\"top_k_retrieve\"],\n",
        "        filter=filters,\n",
        "        include_metadata=True\n",
        "    ).matches\n",
        "\n",
        "    # BM25 lexical search\n",
        "    tokenized_query = [t.lower() for t in word_tokenize(query)]\n",
        "    bm25_scores = bm25_index.get_scores(tokenized_query)\n",
        "    bm25_indices = np.argsort(bm25_scores)[-CONFIG[\"top_k_retrieve\"]:][::-1]\n",
        "\n",
        "    # Combine results\n",
        "    combined = []\n",
        "\n",
        "    # Add vector results\n",
        "    for match in vector_results:\n",
        "        metadata = normalize_metadata(match.metadata)\n",
        "        combined.append((metadata, match.score))\n",
        "\n",
        "    # Add BM25 results\n",
        "    for i in bm25_indices:\n",
        "        metadata = chunks_df.iloc[i][\"metadata\"]\n",
        "        combined.append((metadata, bm25_scores[i]))\n",
        "\n",
        "    # Apply post-retrieval boosts\n",
        "    boosted_combined = []\n",
        "    for metadata, score in combined:\n",
        "        boost = 1.0\n",
        "\n",
        "        # Boost based on helpfulness ratio\n",
        "        try:\n",
        "            helpful_ratio = float(metadata.get('helpful_ratio', 0))\n",
        "            boost *= (1 + helpful_ratio * 0.3)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "\n",
        "        # Apply genre preference boost (if available and no errors in first query)\n",
        "        if hasattr(user_preferences, \"genre_preferences\") and user_preferences[\"genre_preferences\"]:\n",
        "            try:\n",
        "                genres = str(metadata.get('genres', '')).split(',')\n",
        "                for genre in genres:\n",
        "                    if genre.strip() in user_preferences[\"genre_preferences\"]:\n",
        "                        boost *= 1.1  # Small boost for preferred genres\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        boosted_combined.append((metadata, score * boost))\n",
        "\n",
        "    # Sort and return top results\n",
        "    sorted_results = sorted(boosted_combined, key=lambda x: x[1], reverse=True)\n",
        "    return [item[0] for item in sorted_results[:CONFIG[\"top_k_final\"]]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_review_insights(context, max_reviews=3):\n",
        "    \"\"\"Extract meaningful insights from the most helpful reviews\"\"\"\n",
        "    if not context:\n",
        "        return \"\"\n",
        "\n",
        "    # Sort context by helpful_ratio\n",
        "    try:\n",
        "        sorted_context = sorted(context,\n",
        "                               key=lambda x: float(x.get('helpful_ratio', 0)),\n",
        "                               reverse=True)\n",
        "    except (ValueError, TypeError):\n",
        "        # Fallback if sorting fails\n",
        "        sorted_context = context\n",
        "\n",
        "    # Get top reviews with high helpful ratio\n",
        "    insights = []\n",
        "    for review in sorted_context:\n",
        "        # Only include if reviewed by multiple people and majority found it helpful\n",
        "        try:\n",
        "            total_votes = int(review.get('raw_helpful', '0/0').split('/')[1])\n",
        "            helpful_ratio = float(review.get('helpful_ratio', 0))\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "        if total_votes >= 5 and helpful_ratio >= 0.7:\n",
        "            review_text = review.get('text', '')\n",
        "\n",
        "            # Find a meaningful excerpt (limited to 150 chars for conciseness)\n",
        "            excerpt = review_text[:500].strip()\n",
        "            if len(excerpt) < len(review_text):\n",
        "                excerpt += \"...\"\n",
        "\n",
        "            # Format with helpful information\n",
        "            insight = f\"'{excerpt}' - Review rated helpful by {int(helpful_ratio * 100)}% of {total_votes} viewers\"\n",
        "            insights.append(insight)\n",
        "\n",
        "            if len(insights) >= max_reviews:\n",
        "                break\n",
        "\n",
        "    if insights:\n",
        "        return \"\\n\\nHelpful Review Highlights:\\n• \" + \"\\n• \".join(insights)\n",
        "    else:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "mON9k3NmDQJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_diversity_reranking(results, scores, query_intent):\n",
        "    \"\"\"Promotes diversity in results based on query intent\"\"\"\n",
        "    # Get initial top item\n",
        "    top_indices = [np.argmax(scores)]\n",
        "    remaining = list(range(len(scores)))\n",
        "    remaining.remove(top_indices[0])\n",
        "\n",
        "    # Set diversity factor based on query\n",
        "    diversity_factor = 0.3\n",
        "    if query_intent.get('wants_variety', False):\n",
        "        diversity_factor = 0.5\n",
        "\n",
        "    # Add items with penalty for similarity to already selected\n",
        "    while len(top_indices) < min(CONFIG[\"top_k_final\"], len(scores)):\n",
        "        best_idx = -1\n",
        "        best_score = -float('inf')\n",
        "\n",
        "        for idx in remaining:\n",
        "            # Base score\n",
        "            score = scores[idx]\n",
        "\n",
        "            # Diversity penalty based on similarity to selected items\n",
        "            penalty = 0\n",
        "            for selected in top_indices:\n",
        "                sim = compute_film_similarity(results[idx][0], results[selected][0])\n",
        "                penalty += sim\n",
        "\n",
        "            penalty /= len(top_indices)\n",
        "            adjusted_score = score * (1 - diversity_factor * penalty)\n",
        "\n",
        "            if adjusted_score > best_score:\n",
        "                best_score = adjusted_score\n",
        "                best_idx = idx\n",
        "\n",
        "        if best_idx != -1:\n",
        "            top_indices.append(best_idx)\n",
        "            remaining.remove(best_idx)\n",
        "\n",
        "    return [results[i][0] for i in top_indices]"
      ],
      "metadata": {
        "id": "K1jYHTn6qeuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAjPK__HqYX0"
      },
      "outputs": [],
      "source": [
        "def build_response_prompt(query, context):\n",
        "    \"\"\"Structure the LLM prompt for coherent responses with review insights\"\"\"\n",
        "    # Basic context information\n",
        "    context_str = \"\\n\\n\".join(\n",
        "        f\"Title: {ctx['title']} ({ctx.get('year', 'N/A')})\\n\"\n",
        "        f\"Type: {ctx.get('content_type', 'movie')}\\n\"\n",
        "        f\"Director: {ctx.get('director', 'Unknown')}\\n\"\n",
        "        f\"Cast: {ctx.get('cast', 'N/A')}\\n\"\n",
        "        f\"IMDB: {ctx.get('imdb_rating', 0.0)}/10 | \"\n",
        "        f\"User Rating: {ctx.get('user_rating', 0.0)}/10\\n\"\n",
        "        f\"Review Helpfulness: {ctx.get('raw_helpful', '0/0')} ({int(float(ctx.get('helpful_ratio', 0)) * 100)}% positive)\\n\"\n",
        "        f\"Excerpt: {ctx.get('text', '')[:200]}...\"\n",
        "        for ctx in context\n",
        "    )\n",
        "\n",
        "    # Extract review insights\n",
        "    review_insights = extract_review_insights(context)\n",
        "\n",
        "    # Add conversation history context\n",
        "    conversation_context = \"\"\n",
        "    if 'conversation_history' in user_preferences and user_preferences[\"conversation_history\"]:\n",
        "        recent = user_preferences[\"conversation_history\"][-3:]  # Last 3 exchanges\n",
        "        conversation_context = \"Recent conversation:\\n\" + \"\\n\".join(\n",
        "            [f\"User: {ex['query']}\\nCinematicAI: {ex['response'][:150]}\" for ex in recent]\n",
        "        )\n",
        "\n",
        "    # Add user preference context\n",
        "    user_context = \"\"\n",
        "    if 'genre_preferences' in user_preferences and user_preferences[\"genre_preferences\"]:\n",
        "        # Get top genres\n",
        "        top_genres = sorted(user_preferences[\"genre_preferences\"].items(),\n",
        "                           key=lambda x: x[1], reverse=True)[:3]\n",
        "        genre_str = \", \".join([g[0] for g in top_genres]) if top_genres else \"Not enough data\"\n",
        "\n",
        "        # Get recent watches\n",
        "        recent_watches = user_preferences[\"watch_history\"][-3:] if user_preferences[\"watch_history\"] else []\n",
        "        watch_str = \", \".join(recent_watches) if recent_watches else \"No recent history\"\n",
        "\n",
        "        user_context = f\"\"\"\n",
        "User Preferences:\n",
        "- Favorite Genres: {genre_str}\n",
        "- Recently Discussed: {watch_str}\n",
        "\"\"\"\n",
        "\n",
        "    # Complete prompt\n",
        "    return f\"\"\"<|system|>\n",
        "You are CinematicAI, a conversational AI expert on films and TV shows. Provide personalized recommendations\n",
        "and insights based on the context and user preferences. Maintain a natural, engaging conversational style.\n",
        "\n",
        "{user_context}\n",
        "{conversation_context}\n",
        "\n",
        "<|user|>\n",
        "{query}\n",
        "\n",
        "Context Films and Shows:\n",
        "{context_str}{review_insights}\n",
        "\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "async def generate_response(query, context):\n",
        "    \"\"\"Generate response with conversation tracking\"\"\"\n",
        "    prompt = build_response_prompt(query, context)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    generation_config = {\n",
        "        \"max_new_tokens\": 500,  # Increased from default\n",
        "        \"temperature\": 0.4,\n",
        "        \"top_p\": 0.85,\n",
        "        \"do_sample\": True,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"use_cache\": False  # Disable cache to avoid errors\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **generation_config)\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract just the assistant's response\n",
        "    if \"<|assistant|>\" in full_response:\n",
        "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        response = full_response.split(\"Context Films and Shows:\")[-1].strip()\n",
        "\n",
        "    # Update user preferences with this interaction\n",
        "    update_user_preferences(query, response, context)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjEpeYy6uOjx"
      },
      "source": [
        "#Main Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_YC63Fjs_dc"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    \"\"\"End-to-end processing pipeline\"\"\"\n",
        "    try:\n",
        "        pinecone_index = initialize_pinecone()\n",
        "        stats = pinecone_index.describe_index_stats()\n",
        "        csv_path = \"/content/drive/MyDrive/Colab Notebooks/reviews.csv\"\n",
        "\n",
        "        # Check if first run or missing files\n",
        "        first_run = (\n",
        "            stats['total_vector_count'] == 0 or\n",
        "            not os.path.exists(CONFIG[\"bm25_path\"]) or\n",
        "            not os.path.exists(CONFIG[\"metadata_path\"])\n",
        "        )\n",
        "\n",
        "        if first_run:\n",
        "            # Your existing first-run data processing code\n",
        "            pass\n",
        "        else:\n",
        "            print(\"Loading existing resources...\")\n",
        "            import joblib\n",
        "            bm25_index = joblib.load(CONFIG[\"bm25_path\"])\n",
        "            full_data = pd.read_parquet(CONFIG[\"metadata_path\"])\n",
        "            print(\"Successfully loaded cached resources\")\n",
        "\n",
        "        # Initialize global preferences dict\n",
        "        global user_preferences\n",
        "        user_preferences = {\n",
        "            \"watch_history\": [],\n",
        "            \"genre_preferences\": {},\n",
        "            \"director_preferences\": {},\n",
        "            \"actor_preferences\": {},\n",
        "            \"conversation_history\": []\n",
        "        }\n",
        "\n",
        "        # Set 1: Recommendation-Focused Queries\n",
        "        recommendation_queries = [\n",
        "            \"I loved Inception. What other Christopher Nolan films might I enjoy and why?\",\n",
        "            \"Tell me more about the cast of that movie\",\n",
        "            \"I enjoy comedies with romance. What should I watch next?\",\n",
        "            \"Are there any TV shows similar to that?\",\n",
        "            \"What are the best sci-fi films from the last 5 years?\",\n",
        "            \"I'm in the mood for something with great visual effects but also an emotional story\",\n",
        "            \"What's a good thriller to watch with my family? Nothing too violent please\",\n",
        "            \"I liked Breaking Bad. What other critically acclaimed TV dramas should I watch?\",\n",
        "            \"What are some feel-good movies for a rainy day?\",\n",
        "            \"Show me action movies with strong female leads\",\n",
        "            \"I'm planning a movie marathon. What are the essential films from the Marvel universe?\",\n",
        "            \"What scary movies are actually worth watching according to viewer ratings?\"\n",
        "        ]\n",
        "\n",
        "        # Set 2: Film Knowledge and Analysis Queries\n",
        "        film_knowledge_queries = [\n",
        "            \"How did Robert De Niro's acting style evolve throughout his career?\",\n",
        "            \"Can you explain what makes Citizen Kane so influential?\",\n",
        "            \"What were the major innovations in cinematography during the New Hollywood era?\",\n",
        "            \"How do Quentin Tarantino's films use music?\",\n",
        "            \"What's the difference between film noir and neo-noir?\",\n",
        "            \"Explain the Bechdel test and its significance in film criticism\",\n",
        "            \"How has CGI technology changed filmmaking since the 1990s?\",\n",
        "            \"What are some examples of practical effects that still hold up today?\",\n",
        "            \"Who were the most influential directors of the French New Wave and why?\",\n",
        "            \"How do South Korean films like Parasite use social commentary?\",\n",
        "            \"Compare the directing styles of Martin Scorsese and Steven Spielberg\",\n",
        "            \"What makes a good screenplay according to film critics?\"\n",
        "        ]\n",
        "\n",
        "        # Choose which query set to run (or run both)\n",
        "        query_mode = \"both\"  # Options: \"recommendations\", \"knowledge\", \"both\"\n",
        "\n",
        "        if query_mode in [\"recommendations\", \"both\"]:\n",
        "            print(\"\\n\\n\" + \"=\"*20 + \" RECOMMENDATION QUERIES \" + \"=\"*20 + \"\\n\")\n",
        "            # Reset preferences between sets\n",
        "            user_preferences = {\n",
        "                \"watch_history\": [],\n",
        "                \"genre_preferences\": {},\n",
        "                \"director_preferences\": {},\n",
        "                \"actor_preferences\": {},\n",
        "                \"conversation_history\": []\n",
        "            }\n",
        "\n",
        "            for query in recommendation_queries:\n",
        "                print(f\"\\n{'='*40}\\nUser: {query}\")\n",
        "                try:\n",
        "                    context = await hybrid_search(query, pinecone_index, bm25_index, full_data)\n",
        "                    response = await generate_response(query, context)\n",
        "                    print(f\"\\nCinematicAI:\\n{response}\\n{'='*40}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Query failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        if query_mode in [\"knowledge\", \"both\"]:\n",
        "            print(\"\\n\\n\" + \"=\"*20 + \" FILM KNOWLEDGE QUERIES \" + \"=\"*20 + \"\\n\")\n",
        "            # Reset preferences between sets\n",
        "            user_preferences = {\n",
        "                \"watch_history\": [],\n",
        "                \"genre_preferences\": {},\n",
        "                \"director_preferences\": {},\n",
        "                \"actor_preferences\": {},\n",
        "                \"conversation_history\": []\n",
        "            }\n",
        "\n",
        "            for query in film_knowledge_queries:\n",
        "                print(f\"\\n{'='*40}\\nUser: {query}\")\n",
        "                try:\n",
        "                    context = await hybrid_search(query, pinecone_index, bm25_index, full_data)\n",
        "                    response = await generate_response(query, context)\n",
        "                    print(f\"\\nCinematicAI:\\n{response}\\n{'='*40}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Query failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        print(\"\\n\\nFinal verification:\")\n",
        "        print(f\"- Pinecone vectors: {pinecone_index.describe_index_stats().get('total_vector_count', 0)}\")\n",
        "        print(f\"- BM25 exists: {os.path.exists(CONFIG['bm25_path'])}\")\n",
        "        print(f\"- Metadata exists: {os.path.exists(CONFIG['metadata_path'])}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rag_system(test_queries, ground_truth=None):\n",
        "    \"\"\"Evaluate the RAG system performance\"\"\"\n",
        "    results = {\n",
        "        'retrieval_metrics': {\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'ndcg': []\n",
        "        },\n",
        "        'response_metrics': {\n",
        "            'relevance': [],\n",
        "            'factual_accuracy': [],\n",
        "            'recommendation_quality': []\n",
        "        },\n",
        "        'efficiency_metrics': {\n",
        "            'retrieval_time': [],\n",
        "            'generation_time': []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for query in test_queries:\n",
        "        # Measure retrieval performance\n",
        "        start_time = time.time()\n",
        "        context = asyncio.run(hybrid_search(query, index, bm25_index, full_data))\n",
        "        retrieval_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate retrieval against ground truth if available\n",
        "        if ground_truth and query in ground_truth:\n",
        "            retrieval_metrics = evaluate_retrieval(context, ground_truth[query])\n",
        "            results['retrieval_metrics']['precision'].append(retrieval_metrics['precision'])\n",
        "            results['retrieval_metrics']['recall'].append(retrieval_metrics['recall'])\n",
        "            results['retrieval_metrics']['ndcg'].append(retrieval_metrics['ndcg'])\n",
        "\n",
        "        # Measure response generation\n",
        "        start_time = time.time()\n",
        "        response = asyncio.run(generate_response(query, context))\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Record timing metrics\n",
        "        results['efficiency_metrics']['retrieval_time'].append(retrieval_time)\n",
        "        results['efficiency_metrics']['generation_time'].append(generation_time)\n",
        "\n",
        "        # Evaluate response quality if ground truth available\n",
        "        if ground_truth and query in ground_truth:\n",
        "            response_metrics = evaluate_response_quality(response, ground_truth[query])\n",
        "            for metric in response_metrics:\n",
        "                results['response_metrics'][metric].append(response_metrics[metric])\n",
        "\n",
        "    # Aggregate results\n",
        "    for category in results:\n",
        "        for metric in results[category]:\n",
        "            if results[category][metric]:\n",
        "                results[category][metric] = {\n",
        "                    'mean': np.mean(results[category][metric]),\n",
        "                    'std': np.std(results[category][metric]),\n",
        "                    'min': min(results[category][metric]),\n",
        "                    'max': max(results[category][metric])\n",
        "                }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cBJt_Ckbqu_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xG0XGmQ6ySO"
      },
      "source": [
        "#Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDexk9MMudSU"
      },
      "outputs": [],
      "source": [
        "nest_asyncio.apply()\n",
        "asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}